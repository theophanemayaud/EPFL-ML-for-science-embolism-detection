{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "U-net_for_project2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7cDqXqaGk_A"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9FxPYSkAq6o"
      },
      "source": [
        "#Cutting image\n",
        "def Cutting(img):\n",
        "  img_size = img.shape\n",
        "  mod1= img.shape[0]-60 % 16\n",
        "  mod2= img.shape[1]-60 % 16\n",
        "  if mod1 != 0:\n",
        "    start=int(np.trunc(mod1)) + (mod1 % 2)\n",
        "    end= img.shape[0]-int(np.trunc(mod1))\n",
        "    new_img=img[start:end-1,:]\n",
        "  if mod2 != 0:\n",
        "    start=int(np.trunc(mod2)) + (mod2 % 2)\n",
        "    end= img.shape[1]-int(np.trunc(mod2))\n",
        "    new_img=new_img[:,start:end-1]\n",
        "  return new_img"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fu9qi9q-Gb7"
      },
      "source": [
        "'''\n",
        "def calc_dim_input(dim_image):\n",
        "   def expand1(x):\n",
        "     return (x+4)/2\n",
        "   def expand2(x):\n",
        "     return (x+4)*2\n",
        "   dimension_input=expand1(expand1(expand1(expand1(dim_image))))\n",
        "   dimension_input=expand2(expand2(expand2(expand2(dimension_input))))\n",
        "   dimension_input=dimension_input+4\n",
        "   return int(dimension_input)\n",
        "calc_dim_input(996)\n",
        " '''   \n",
        "def calc_dim_input(dim_image):\n",
        "  '''\n",
        "    Calculating the dimension of input to have an output image of the same size as the original input image\n",
        "  '''\n",
        "  return dim_image + 184           "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql5oVfN2DjdY"
      },
      "source": [
        "def extend_mirror(img, out_size):\n",
        "    '''\n",
        "    A method to extend an image to certain resolution by mirrorring the edges\n",
        "    Input:\n",
        "    :img: image as numpy array\n",
        "    :out_size: a tuple of the desired output resolution\n",
        "    Output:\n",
        "    :out: the extended image\n",
        "    '''\n",
        "    if np.any(img.shape>out_size):\n",
        "        raise Exception('Error: at least on of out_size axes is smaller than the image shape')\n",
        "    img_size = img.shape\n",
        "    out = np.zeros(out_size)\n",
        "    v_edge_u = (out_size[0]-img_size[0]) // 2\n",
        "    v_edge_d = -(out_size[0]-img_size[0]-v_edge_u)\n",
        "    h_edge_l = (out_size[1]-img_size[1]) // 2\n",
        "    h_edge_r = -(out_size[1]-img_size[1]-h_edge_l)\n",
        "    # centre\n",
        "    out[v_edge_u:v_edge_d,h_edge_l:h_edge_r] = img\n",
        "    # sides\n",
        "    out[:v_edge_u,h_edge_l:h_edge_r] = np.flipud(img[:v_edge_u,:]) # top\n",
        "    out[v_edge_d:,h_edge_l:h_edge_r] = np.flipud(img[v_edge_d:,:]) # bottom\n",
        "    out[v_edge_u:v_edge_d,:h_edge_l] = np.fliplr(img[:,:h_edge_l]) # left\n",
        "    out[v_edge_u:v_edge_d,h_edge_r:] = np.fliplr(img[:,h_edge_r:]) # right\n",
        "    # corners\n",
        "    out[:v_edge_u,:h_edge_l] = np.fliplr(out[:v_edge_u,h_edge_l:h_edge_l*2]) # top-left\n",
        "    out[:v_edge_u,h_edge_r:] = np.fliplr(out[:v_edge_u,2*h_edge_r:h_edge_r]) # top-right\n",
        "    out[v_edge_d:,:h_edge_l] = np.fliplr(out[v_edge_d:,h_edge_l:h_edge_l*2]) # bottom-left\n",
        "    out[v_edge_d:,h_edge_r:] = np.fliplr(out[v_edge_d:,2*h_edge_r:h_edge_r]) # bottom-right\n",
        "    return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG0dPfOwGk_B"
      },
      "source": [
        "Implementing U-net for our problem. Here all the 23 layers are used. Due to the fact that our problem consist in saying if a pixel is part or not of an air bubble the final output is given by tensor with two channels, each one corresponding to a matrix of dimensions equal to the original image. Using many convolutional layers the final output has a lower dimension than the input. In order to obtained the desired output the input image is expanded. The parts added are obtained mirroring parts of the real image as described in the paper. \n",
        "This tecnique is used in the training of NN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3heSHImrGk_B"
      },
      "source": [
        "class U_net(nn.Module):\n",
        "    def __init__(self, n_channels=64):\n",
        "        'U-net from the paper \"Olaf Ronneberger, Philipp Fischer, and Thomas Brox\": https://arxiv.org/abs/1505.04597 '\n",
        "        super(U_net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,n_channels,3)\n",
        "        self.conv2 = nn.Conv2d(n_channels,n_channels,3)\n",
        "        self.pool = nn.MaxPool2d(2,stride=2)\n",
        "        self.conv3 = nn.Conv2d(n_channels,2*n_channels,3)\n",
        "        self.conv4 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
        "        self.conv5 = nn.Conv2d(2*n_channels,4*n_channels,3)\n",
        "        self.conv6 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
        "        self.conv7 = nn.Conv2d(4*n_channels,8*n_channels,3)\n",
        "        self.conv8 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
        "        self.conv9 = nn.Conv2d(8*n_channels,16*n_channels,3)\n",
        "        self.conv10 = nn.Conv2d(16*n_channels,16*n_channels,3)\n",
        "        #self.upconv1= nn.ConvTranspose2d(16*n_channels,16*n_channels,2)\n",
        "        self.upconv1 = nn.Upsample(scale_factor=2)\n",
        "        self.conv11 = nn.Conv2d(16*n_channels,8*n_channels,3)\n",
        "        self.conv12 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
        "        #self.upconv2= nn.ConvTranspose2d(8*n_channels,8*n_channels,2)\n",
        "        self.upconv2 = nn.Upsample(scale_factor=2)\n",
        "        self.conv13 = nn.Conv2d(8*n_channels,4*n_channels,3)\n",
        "        self.conv14 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
        "        #self.upconv3= nn.ConvTranspose2d(4*n_channels,4*n_channels,2)\n",
        "        self.upconv3 = nn.Upsample(scale_factor=2)\n",
        "        self.conv15 = nn.Conv2d(4*n_channels,2*n_channels,3)\n",
        "        self.conv16 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
        "        #self.upconv4= nn.ConvTranspose2d(2*n_channels,2*n_channels,2)\n",
        "        self.upconv4 = nn.Upsample(scale_factor=2)\n",
        "        self.conv17 = nn.Conv2d(2*n_channels,n_channels,3)\n",
        "        self.conv18 = nn.Conv2d(n_channels,n_channels,3)\n",
        "        self.conv1_1= nn.Conv2d(n_channels,2,1)\n",
        "        self.activation1=nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.pool(self.activation1(self.conv2(self.activation1(self.conv1(x)))))\n",
        "        out = self.pool(self.activation1(self.conv4(self.activation1(self.conv3(out)))))\n",
        "        out = self.pool(self.activation1(self.conv6(self.activation1(self.conv5(out)))))\n",
        "        out = self.pool(self.activation1(self.conv8(self.activation1(self.conv7(out)))))\n",
        "        out = self.activation1(self.conv10(self.activation1(self.conv9(out))))\n",
        "        out = self.upconv1(out)\n",
        "        out = self.activation1(self.conv12(self.activation1(self.conv11(out))))\n",
        "        out = self.upconv2(out)\n",
        "        out = self.activation1(self.conv14(self.activation1(self.conv13(out))))\n",
        "        out = self.upconv3(out)\n",
        "        out = self.activation1(self.conv16(self.activation1(self.conv15(out))))\n",
        "        out = self.upconv4(out)\n",
        "        out = self.activation1(self.conv18(self.activation1(self.conv17(out))))\n",
        "        out = self.conv1_1(out)\n",
        "        return out\n",
        "\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV81YSi0Gk_B"
      },
      "source": [
        "#Accuracy \n",
        "def accuracy(pred, test_labels):\n",
        "    '''\n",
        "    pred: torch.tensor (result of U-net) of size [num_batches=1, 2, dim_image1, dim_image2]\n",
        "    test_labels: torch.tensor (Real labels for the image) \n",
        "    '''\n",
        "    '''\n",
        "    Calculate the percentage of correct pixels labeled\n",
        "    '''\n",
        "    test_labels=test_labels.view(1,test_labels)\n",
        "    label_pred=torch.argmax(pred,dim=1)\n",
        "    acc= (torch.abs(label_pred-test_labels)).mean()\n",
        "    return 1-acc "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5E9lylozMEC"
      },
      "source": [
        "#Training (adapting function from lab10)\n",
        "def train(model, criterion, image_input, labeled_images, optimizer, num_epochs):\n",
        "  \"\"\"\n",
        "  @param model: torch.nn.Module\n",
        "  @param criterion: torch.nn.modules.loss._Loss\n",
        "  @param image_input: numpy.ndarray\n",
        "  @param labeled_images: numpy.ndarray\n",
        "  @param optimizer: torch.optim.Optimizer\n",
        "  @param num_epochs: int\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Starting training\")\n",
        "  #Cycle for epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    # Train an epoch\n",
        "    model.train()\n",
        "    #Training using an image\n",
        "    for x, y in image_input, images_labeled:\n",
        "      #Cutting image and label to have an even correct dimension\n",
        "      x=Cutting(x)\n",
        "      y=Cutting(y)\n",
        "      dimension_input1= calc_dim_input(x.shape[0])\n",
        "      dimension_input2= calc_dim_input(x.shape[1]) \n",
        "      out_size=[dimension_input1,dimension_input2]\n",
        "      #Mirroring like describe in the paper\n",
        "      ext_x = extend_mirror(x, out_size)\n",
        "      ext_x=torch.from_numpy(ext_x)\n",
        "      ext_x=ext_x.view(1,1,dimension_input1,dimension_input2)\n",
        "      y=torch.from_numpy(y)\n",
        "      y=y.view(1,1,dim_image1,dim_image2)\n",
        "\n",
        "      # Evaluate the network (forward pass)\n",
        "      prediction = model(ext_x)\n",
        "      loss = criterion(prediction,y)\n",
        "      \n",
        "      # Compute the gradient\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # Update the parameters of the model with a gradient step\n",
        "      optimizer.step()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEwe6E2gwzys"
      },
      "source": [
        "The loss function used to train the network is CrossEntropy as described in the paper. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "wsZfLMYfNBMS",
        "outputId": "bfd9541c-64a4-46eb-aa1c-28bafad56dfb"
      },
      "source": [
        "num_epochs=10\n",
        "learning_rate=0.001\n",
        "optimizer=torch.optim.Adam(U_net(), lr=learning_rate) #In the paper they use SGD\n",
        "criterion=torch.nn.CrossEntropyLoss() \n",
        "train(U_net(), criterion, image_input, labeled_images, optimizer, num_epochs)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-d82570b274b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#In the paper they use SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     46\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     47\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'U_net' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdqipSh0Gk_B"
      },
      "source": [
        "#Trying NN\n",
        "dim_image1=1000\n",
        "dim_image2=1000\n",
        "image=torch.rand(dim_image1,dim_image2)\n",
        "image=image.view(1,1,dim_image1,dim_image2)\n",
        "model=U_net()\n",
        "a=model(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opmSNsr8Gk_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb733930-4c9f-4b06-f900-52cd4c091b0a"
      },
      "source": [
        "a.size()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 388, 388])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Heg-C3s_dN"
      },
      "source": [
        "To try a different approach we adapt the U-net changing the last layer. In this case the output is given by a tensor with only one channel. After that a sigmoid function is applied in order to have an output value for every pixel between 0 and 1. This value models the probability of a pixel to be part of an air bubble. \n",
        "This is equivalent to do a logistic regression in the last layer, therefore in the training of the model we use 'Bcewithlogitloss' function of pytorch, which gives us logistic loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVBHs2_1ti47"
      },
      "source": [
        "class U_net_with_prob(nn.Module):\n",
        "    def __init__(self, n_channels=64):\n",
        "        'U-net from the paper \"Olaf Ronneberger, Philipp Fischer, and Thomas Brox\": https://arxiv.org/abs/1505.04597 '\n",
        "        super(U_net_with_prob, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,n_channels,3)\n",
        "        self.conv2 = nn.Conv2d(n_channels,n_channels,3)\n",
        "        self.pool = nn.MaxPool2d(2,stride=2)\n",
        "        self.conv3 = nn.Conv2d(n_channels,2*n_channels,3)\n",
        "        self.conv4 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
        "        self.conv5 = nn.Conv2d(2*n_channels,4*n_channels,3)\n",
        "        self.conv6 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
        "        self.conv7 = nn.Conv2d(4*n_channels,8*n_channels,3)\n",
        "        self.conv8 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
        "        self.conv9 = nn.Conv2d(8*n_channels,16*n_channels,3)\n",
        "        self.conv10 = nn.Conv2d(16*n_channels,16*n_channels,3)\n",
        "        #self.upconv1= nn.ConvTranspose2d(16*n_channels,16*n_channels,2)\n",
        "        self.upconv1 = nn.Upsample(scale_factor=2)\n",
        "        self.conv11 = nn.Conv2d(16*n_channels,8*n_channels,3)\n",
        "        self.conv12 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
        "        #self.upconv2= nn.ConvTranspose2d(8*n_channels,8*n_channels,2)\n",
        "        self.upconv2 = nn.Upsample(scale_factor=2)\n",
        "        self.conv13 = nn.Conv2d(8*n_channels,4*n_channels,3)\n",
        "        self.conv14 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
        "        #self.upconv3= nn.ConvTranspose2d(4*n_channels,4*n_channels,2)\n",
        "        self.upconv3 = nn.Upsample(scale_factor=2)\n",
        "        self.conv15 = nn.Conv2d(4*n_channels,2*n_channels,3)\n",
        "        self.conv16 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
        "        #self.upconv4= nn.ConvTranspose2d(2*n_channels,2*n_channels,2)\n",
        "        self.upconv4 = nn.Upsample(scale_factor=2)\n",
        "        self.conv17 = nn.Conv2d(2*n_channels,n_channels,3)\n",
        "        self.conv18 = nn.Conv2d(n_channels,n_channels,3)\n",
        "        self.conv1_1= nn.Conv2d(n_channels,1,1)\n",
        "        self.activation1=nn.ReLU()\n",
        "        self.activation2=nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.pool(self.activation1(self.conv2(self.activation1(self.conv1(x)))))\n",
        "        out = self.pool(self.activation1(self.conv4(self.activation1(self.conv3(out)))))\n",
        "        out = self.pool(self.activation1(self.conv6(self.activation1(self.conv5(out)))))\n",
        "        out = self.pool(self.activation1(self.conv8(self.activation1(self.conv7(out)))))\n",
        "        out = self.activation1(self.conv10(self.activation1(self.conv9(out))))\n",
        "        out = self.upconv1(out)\n",
        "        out = self.activation1(self.conv12(self.activation1(self.conv11(out))))\n",
        "        out = self.upconv2(out)\n",
        "        out = self.activation1(self.conv14(self.activation1(self.conv13(out))))\n",
        "        out = self.upconv3(out)\n",
        "        out = self.activation1(self.conv16(self.activation1(self.conv15(out))))\n",
        "        out = self.upconv4(out)\n",
        "        out = self.activation1(self.conv18(self.activation1(self.conv17(out))))\n",
        "        out = self.activation2(self.conv1_1(out))\n",
        "        return out\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EarVYw2CuZFt"
      },
      "source": [
        "num_epochs=10\n",
        "learning_rate=0.001\n",
        "optimizer=torch.optim.Adam(U_net_with_prob, lr=learning_rate) \n",
        "criterion=nn.BCEWithLogitsLoss( reduction = 'mean')\n",
        "train(U_net_with_prob, criterion, image_input, labeled_images, optimizer, num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1H2KSLxz_nS"
      },
      "source": [
        "#Trying NN\n",
        "dim_image1=1000\n",
        "dim_image2=1000\n",
        "image=torch.rand(dim_image1,dim_image2)\n",
        "image=image.view(1,1,dim_image1,dim_image2)\n",
        "model=U_net_with_prob()\n",
        "a=model(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo71Ajzi0b4w"
      },
      "source": [
        "a.size()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}