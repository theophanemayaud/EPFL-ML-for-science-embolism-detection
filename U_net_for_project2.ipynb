{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M7cDqXqaGk_A"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import os\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_names = glob.glob('./th_analysedimages/*.tif')\n",
    "imgs = [cv.imread(name, cv.IMREAD_UNCHANGED) for name in imgs_names]\n",
    "len(imgs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_names = glob.glob('**/labels/*.png')\n",
    "labels = [np.max(cv.imread(name, cv.IMREAD_UNCHANGED),axis=2) for name in labels_names]\n",
    "len(labels_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1233, 1236)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0].shape\n",
    "#Cutting(labels[0]).shape\n",
    "a=labels[0]\n",
    "a[3:783,:].shape\n",
    "labels[255].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i9FxPYSkAq6o"
   },
   "outputs": [],
   "source": [
    "#Cutting image\n",
    "def Cutting(img):\n",
    "  img_size = img.shape\n",
    "  mod1= (img.shape[0]-60) % 16\n",
    "  mod2= (img.shape[1]-60) % 16\n",
    "  new_img=img\n",
    "  if mod1 != 0:\n",
    "    start=int(np.trunc(mod1/2)) + (mod1 % 2)\n",
    "    end= img.shape[0]-int(np.trunc(mod1/2))\n",
    "    new_img=new_img[start:end,:]\n",
    "  if mod2 != 0:\n",
    "    start=int(np.trunc(mod2/2)) + (mod2 % 2)\n",
    "    end= img.shape[1]-int(np.trunc(mod2/2))\n",
    "    new_img=new_img[:,start:end]\n",
    "  return new_img, mod1, \n",
    "#Function to cut label image\n",
    "#It can the image of cut1 and cut2 pixels\n",
    "def cut_y(img, cut1, cut2):\n",
    "    new_img=img\n",
    "    if cut1 != 0:\n",
    "        start=int(np.trunc(cut1/2)) + (cut1 % 2)\n",
    "        end= img.shape[0]-int(np.trunc(cut1/2))\n",
    "        new_img=new_img[start:end,:]\n",
    "    if cut2 != 0:\n",
    "        start=int(np.trunc(cut2/2)) + (cut2 % 2)\n",
    "        end= img.shape[1]-int(np.trunc(cut2/2))\n",
    "        new_img=new_img[:,start:end]\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9Fu9qi9q-Gb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ndef calc_dim_input(dim_image):\\n  return dim_image + 184\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def calc_dim_input(dim_image):\n",
    "    \n",
    "    #'''\n",
    "    #Calculating the dimension of input to have an output image of the same size as the original input image\n",
    "    #'''\n",
    "    #def expand1(x):\n",
    "        #return (x+4)/2\n",
    "    #def expand2(x):\n",
    "        #return (x+4)*2\n",
    "    #dimension_input=expand1(expand1(expand1(expand1(dim_image))))\n",
    "    #dimension_input=expand2(expand2(expand2(expand2(dimension_input))))\n",
    "    #dimension_input=dimension_input+4\n",
    "    #return int(dimension_input)\n",
    "#calc_dim_input(996) \n",
    "def calc_dim_input(dim_image):\n",
    "  return dim_image + 184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ql5oVfN2DjdY"
   },
   "outputs": [],
   "source": [
    "def extend_mirror(img, out_size):\n",
    "    '''\n",
    "    A method to extend an image to certain resolution by mirrorring the edges\n",
    "    Input:\n",
    "    :img: image as numpy array\n",
    "    :out_size: a tuple of the desired output resolution\n",
    "    Output:\n",
    "    :out: the extended image\n",
    "    '''\n",
    "    # input error exceptions\n",
    "    if np.any(img.shape>tuple(out_size)):\n",
    "        raise Exception('Error: at least on of out_size axes is smaller than the image shape')\n",
    "    if np.any(3*img.shape>tuple(out_size)):\n",
    "        raise Exception('Error: at least on of out_size axes is at least 3 times larger than the image shape')\n",
    "    # output parameters\n",
    "    out = np.zeros(out_size)\n",
    "    v_edge_u = (out_size[0]-img.shape[0]) // 2\n",
    "    v_edge_d = -(out_size[0]-img.shape[0]-v_edge_u)\n",
    "    h_edge_l = (out_size[1]-img.shape[1]) // 2\n",
    "    h_edge_r = -(out_size[1]-img.shape[1]-h_edge_l)\n",
    "    # output centre\n",
    "    out[v_edge_u:v_edge_d,h_edge_l:h_edge_r] = img\n",
    "    # output sides\n",
    "    out[:v_edge_u,h_edge_l:h_edge_r] = np.flipud(img[:v_edge_u,:]) # top\n",
    "    out[v_edge_d:,h_edge_l:h_edge_r] = np.flipud(img[v_edge_d:,:]) # bottom\n",
    "    out[v_edge_u:v_edge_d,:h_edge_l] = np.fliplr(img[:,:h_edge_l]) # left\n",
    "    out[v_edge_u:v_edge_d,h_edge_r:] = np.fliplr(img[:,h_edge_r:]) # right\n",
    "    # output corners\n",
    "    out[:v_edge_u,:h_edge_l] = np.fliplr(out[:v_edge_u,h_edge_l:h_edge_l*2]) # top-left\n",
    "    out[:v_edge_u,h_edge_r:] = np.fliplr(out[:v_edge_u,2*h_edge_r:h_edge_r]) # top-right\n",
    "    out[v_edge_d:,:h_edge_l] = np.fliplr(out[v_edge_d:,h_edge_l:h_edge_l*2]) # bottom-left\n",
    "    out[v_edge_d:,h_edge_r:] = np.fliplr(out[v_edge_d:,2*h_edge_r:h_edge_r]) # bottom-right\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG0dPfOwGk_B"
   },
   "source": [
    "Implementing U-net for our problem. Here all the 23 layers are used. Due to the fact that our problem consist in saying if a pixel is part or not of an air bubble the final output is given by tensor with two channels, each one corresponding to a matrix of dimensions equal to the original image. Using many convolutional layers the final output has a lower dimension than the input. In order to obtained the desired output the input image is expanded. The parts added are obtained mirroring parts of the real image as described in the paper. \n",
    "This tecnique is used in the training of NN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GV81YSi0Gk_B"
   },
   "outputs": [],
   "source": [
    "#Accuracy \n",
    "def accuracy(pred, test_labels):\n",
    "    '''\n",
    "    pred: torch.tensor (result of U-net) of size [num_batches=1, 2, dim_image1, dim_image2]\n",
    "    test_labels: torch.tensor (Real labels for the image) \n",
    "    '''\n",
    "    '''\n",
    "    Calculate the percentage of correct pixels labeled\n",
    "    '''\n",
    "    test_labels=test_labels.view(1,test_labels)\n",
    "    label_pred=torch.argmax(pred,dim=1)\n",
    "    acc= (torch.abs(label_pred-test_labels)).mean()\n",
    "    return 1-acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_net(nn.Module):\n",
    "    def __init__(self, n_channels=64):\n",
    "        'U-net from the paper \"Olaf Ronneberger, Philipp Fischer, and Thomas Brox\": https://arxiv.org/abs/1505.04597 '\n",
    "        super(U_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,n_channels,3)\n",
    "        self.conv2 = nn.Conv2d(n_channels,n_channels,3)\n",
    "        self.pool = nn.MaxPool2d(2,stride=2)\n",
    "        self.conv3 = nn.Conv2d(n_channels,2*n_channels,3)\n",
    "        self.conv4 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
    "        self.conv5 = nn.Conv2d(2*n_channels,4*n_channels,3)\n",
    "        self.conv6 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
    "        self.conv7 = nn.Conv2d(4*n_channels,8*n_channels,3)\n",
    "        self.conv8 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
    "        self.conv9 = nn.Conv2d(8*n_channels,16*n_channels,3)\n",
    "        self.conv10 = nn.Conv2d(16*n_channels,16*n_channels,3)\n",
    "        #self.upconv1= nn.ConvTranspose2d(16*n_channels,16*n_channels,2)\n",
    "        self.upconv1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv11 = nn.Conv2d(16*n_channels,8*n_channels,3)\n",
    "        self.conv12 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
    "        #self.upconv2= nn.ConvTranspose2d(8*n_channels,8*n_channels,2)\n",
    "        self.upconv2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv13 = nn.Conv2d(8*n_channels,4*n_channels,3)\n",
    "        self.conv14 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
    "        #self.upconv3= nn.ConvTranspose2d(4*n_channels,4*n_channels,2)\n",
    "        self.upconv3 = nn.Upsample(scale_factor=2)\n",
    "        self.conv15 = nn.Conv2d(4*n_channels,2*n_channels,3)\n",
    "        self.conv16 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
    "        #self.upconv4= nn.ConvTranspose2d(2*n_channels,2*n_channels,2)\n",
    "        self.upconv4 = nn.Upsample(scale_factor=2)\n",
    "        self.conv17 = nn.Conv2d(2*n_channels,n_channels,3)\n",
    "        self.conv18 = nn.Conv2d(n_channels,n_channels,3)\n",
    "        self.conv1_1= nn.Conv2d(n_channels,2,1)\n",
    "        self.activation1=nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.pool(self.activation1(self.conv2(self.activation1(self.conv1(x)))))\n",
    "        out = self.pool(self.activation1(self.conv4(self.activation1(self.conv3(out)))))\n",
    "        out = self.pool(self.activation1(self.conv6(self.activation1(self.conv5(out)))))\n",
    "        out = self.pool(self.activation1(self.conv8(self.activation1(self.conv7(out)))))\n",
    "        out = self.activation1(self.conv10(self.activation1(self.conv9(out))))\n",
    "        out = self.upconv1(out)\n",
    "        out = self.activation1(self.conv12(self.activation1(self.conv11(out))))\n",
    "        out = self.upconv2(out)\n",
    "        out = self.activation1(self.conv14(self.activation1(self.conv13(out))))\n",
    "        out = self.upconv3(out)\n",
    "        out = self.activation1(self.conv16(self.activation1(self.conv15(out))))\n",
    "        out = self.upconv4(out)\n",
    "        out = self.activation1(self.conv18(self.activation1(self.conv17(out))))\n",
    "        out = self.conv1_1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "f5E9lylozMEC"
   },
   "outputs": [],
   "source": [
    "#Training (adapting function from lab10)\n",
    "def train(model, criterion, image_input, labeled_images, optimizer, num_epochs):\n",
    "  \"\"\"\n",
    "  @param model: torch.nn.Module\n",
    "  @param criterion: torch.nn.modules.loss._Loss\n",
    "  @param image_input: numpy.ndarray\n",
    "  @param labeled_images: numpy.ndarray\n",
    "  @param optimizer: torch.optim.Optimizer\n",
    "  @param num_epochs: int\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"Starting training\")\n",
    "  #Cycle for epochs\n",
    "  for epoch in range(num_epochs):\n",
    "    # Train an epoch\n",
    "    model.train()\n",
    "    #Training using an image\n",
    "    for x, y in image_input, images_labeled:\n",
    "      #Cutting image and label to have an even correct dimension\n",
    "      x=Cutting(x)\n",
    "      y=Cutting(y)\n",
    "      dimension_input1= calc_dim_input(x.shape[0])\n",
    "      dimension_input2= calc_dim_input(x.shape[1]) \n",
    "      out_size=[dimension_input1,dimension_input2]\n",
    "      #Mirroring like describe in the paper\n",
    "      ext_x = extend_mirror(x, out_size)\n",
    "      ext_x=torch.from_numpy(ext_x)\n",
    "      ext_x=ext_x.view(1,1,dimension_input1,dimension_input2)\n",
    "      y=torch.from_numpy(y)\n",
    "      y=y.view(1,1,dim_image1,dim_image2)\n",
    "\n",
    "      # Evaluate the network (forward pass)\n",
    "      prediction = model(ext_x)\n",
    "      loss = criterion(prediction,y)\n",
    "      \n",
    "      # Compute the gradient\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "\n",
    "      # Update the parameters of the model with a gradient step\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEwe6E2gwzys"
   },
   "source": [
    "The loss function used to train the network is CrossEntropy as described in the paper. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "wsZfLMYfNBMS",
    "outputId": "bfd9541c-64a4-46eb-aa1c-28bafad56dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "785\n",
      "785\n",
      "torch.Size([772, 756])\n",
      "torch.Size([1, 1, 956, 940])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 764346368 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-92c6ba8d8ac8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-794acee3bb66>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, criterion, image_input, labeled_images, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Evaluate the network (forward pass)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-1d1ad1dd76c4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv15\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupconv4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv18\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv17\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[0;32m   3130\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample_nearest1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3131\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nearest'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample_nearest2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nearest'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample_nearest3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 764346368 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "learning_rate=0.001\n",
    "model=U_net()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate) #In the paper they use SGD\n",
    "criterion=torch.nn.CrossEntropyLoss() \n",
    "for p in model.parameters():\n",
    "    p.data = p.data.type(torch.DoubleTensor)\n",
    "#train(model, criterion, imgs, labels, optimizer, num_epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdqipSh0Gk_B"
   },
   "outputs": [],
   "source": [
    "#Trying NN\n",
    "dim_image1=1000\n",
    "dim_image2=1000\n",
    "image=torch.rand(dim_image1,dim_image2)\n",
    "image=image.view(1,1,dim_image1,dim_image2)\n",
    "model=U_net()\n",
    "a=model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opmSNsr8Gk_C",
    "outputId": "cb733930-4c9f-4b06-f900-52cd4c091b0a"
   },
   "outputs": [],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_Heg-C3s_dN"
   },
   "source": [
    "To try a different approach we adapt the U-net changing the last layer. In this case the output is given by a tensor with only one channel. After that a sigmoid function is applied in order to have an output value for every pixel between 0 and 1. This value models the probability of a pixel to be part of an air bubble. \n",
    "This is equivalent to do a logistic regression in the last layer, therefore in the training of the model we use 'Bcewithlogitloss' function of pytorch, which gives us logistic loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVBHs2_1ti47"
   },
   "outputs": [],
   "source": [
    "class U_net_with_prob(nn.Module):\n",
    "    def __init__(self, n_channels=64):\n",
    "        'U-net from the paper \"Olaf Ronneberger, Philipp Fischer, and Thomas Brox\": https://arxiv.org/abs/1505.04597 '\n",
    "        super(U_net_with_prob, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,n_channels,3)\n",
    "        self.conv2 = nn.Conv2d(n_channels,n_channels,3)\n",
    "        self.pool = nn.MaxPool2d(2,stride=2)\n",
    "        self.conv3 = nn.Conv2d(n_channels,2*n_channels,3)\n",
    "        self.conv4 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
    "        self.conv5 = nn.Conv2d(2*n_channels,4*n_channels,3)\n",
    "        self.conv6 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
    "        self.conv7 = nn.Conv2d(4*n_channels,8*n_channels,3)\n",
    "        self.conv8 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
    "        self.conv9 = nn.Conv2d(8*n_channels,16*n_channels,3)\n",
    "        self.conv10 = nn.Conv2d(16*n_channels,16*n_channels,3)\n",
    "        #self.upconv1= nn.ConvTranspose2d(16*n_channels,16*n_channels,2)\n",
    "        self.upconv1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv11 = nn.Conv2d(16*n_channels,8*n_channels,3)\n",
    "        self.conv12 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
    "        #self.upconv2= nn.ConvTranspose2d(8*n_channels,8*n_channels,2)\n",
    "        self.upconv2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv13 = nn.Conv2d(8*n_channels,4*n_channels,3)\n",
    "        self.conv14 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
    "        #self.upconv3= nn.ConvTranspose2d(4*n_channels,4*n_channels,2)\n",
    "        self.upconv3 = nn.Upsample(scale_factor=2)\n",
    "        self.conv15 = nn.Conv2d(4*n_channels,2*n_channels,3)\n",
    "        self.conv16 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
    "        #self.upconv4= nn.ConvTranspose2d(2*n_channels,2*n_channels,2)\n",
    "        self.upconv4 = nn.Upsample(scale_factor=2)\n",
    "        self.conv17 = nn.Conv2d(2*n_channels,n_channels,3)\n",
    "        self.conv18 = nn.Conv2d(n_channels,n_channels,3)\n",
    "        self.conv1_1= nn.Conv2d(n_channels,1,1)\n",
    "        self.activation1=nn.ReLU()\n",
    "        self.activation2=nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.pool(self.activation1(self.conv2(self.activation1(self.conv1(x)))))\n",
    "        out = self.pool(self.activation1(self.conv4(self.activation1(self.conv3(out)))))\n",
    "        out = self.pool(self.activation1(self.conv6(self.activation1(self.conv5(out)))))\n",
    "        out = self.pool(self.activation1(self.conv8(self.activation1(self.conv7(out)))))\n",
    "        out = self.activation1(self.conv10(self.activation1(self.conv9(out))))\n",
    "        out = self.upconv1(out)\n",
    "        out = self.activation1(self.conv12(self.activation1(self.conv11(out))))\n",
    "        out = self.upconv2(out)\n",
    "        out = self.activation1(self.conv14(self.activation1(self.conv13(out))))\n",
    "        out = self.upconv3(out)\n",
    "        out = self.activation1(self.conv16(self.activation1(self.conv15(out))))\n",
    "        out = self.upconv4(out)\n",
    "        out = self.activation1(self.conv18(self.activation1(self.conv17(out))))\n",
    "        out = self.activation2(self.conv1_1(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EarVYw2CuZFt"
   },
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "learning_rate=0.001\n",
    "optimizer=torch.optim.Adam(U_net_with_prob, lr=learning_rate) \n",
    "criterion=nn.BCEWithLogitsLoss( reduction = 'mean')\n",
    "train(U_net_with_prob, criterion, image_input, labeled_images, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1H2KSLxz_nS"
   },
   "outputs": [],
   "source": [
    "#Trying NN\n",
    "dim_image1=1000\n",
    "dim_image2=1000\n",
    "image=torch.rand(dim_image1,dim_image2)\n",
    "image=image.view(1,1,dim_image1,dim_image2)\n",
    "model=U_net_with_prob()\n",
    "a=model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo71Ajzi0b4w"
   },
   "outputs": [],
   "source": [
    "a.size()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "U-net_for_project2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
