{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKbc3s9LoKC8"
   },
   "source": [
    "# UNet implementation with complementary tools to try different approaches\n",
    "Based on https://youtu.be/u1loyDCoGbE for implementation of U-net : https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "This notebook is the reference notebook of our implementation, and we will make copies of it, to test different small changes and compare their results.\n",
    "\n",
    "See cell 6 for general parameters which will change frequently. Otherwise changes will be details here in this cell.\n",
    "\n",
    "## Specific test details\n",
    "\n",
    "Will try as name suggests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Platform install of dependencies\n",
    "# %pip install numpy\n",
    "# %pip install torch\n",
    "# %pip install glob\n",
    "# %pip install opencv-python\n",
    "# %pip install time\n",
    "# %pip install matplotlib.\n",
    "# %pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dtCj1z1-oKDE"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import cv2 as cv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3JLUi4s1Dkzf"
   },
   "outputs": [],
   "source": [
    "env = \"gcp\" # local or colab or gcp\n",
    "train_or_load = \"load\" # train or load : Are we training a model, or just loading a pretrained one ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_FzWBgbHoKDF",
    "outputId": "c4d30fb9-5946-48ba-876e-a6a0d8bf88af"
   },
   "outputs": [],
   "source": [
    "#Google Colab specifics\n",
    "if env == \"colab\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    !cp \"/content/drive/MyDrive/helpers.py\" .\n",
    "    imgs_names = glob.glob( '/content/drive/MyDrive/th_analysedimages/*.tif')\n",
    "    labels_names = glob.glob('/content/drive/MyDrive/labels/*.png')\n",
    "elif env == \"gcp\":\n",
    "    imgs_names = glob.glob( '/home/jupyter/cs-433-project-2-ml_fools/th_analysedimages/*.tif')\n",
    "    labels_names = glob.glob('/home/jupyter/cs-433-project-2-ml_fools/th_csv_labels/png_masks_emb/*.png')\n",
    "    !cp \"/home/jupyter/cs-433-project-2-ml_fools/helpers.py\" .\n",
    "#Local github project specifics\n",
    "elif env == \"local\":\n",
    "    imgs_names = glob.glob( '/Users/theophanemayaud/Dev/EPFL MA1/Machine Learning/cs-433-project-2-ml_fools/th_analysedimages/*.tif')\n",
    "    labels_names = glob.glob('/Users/theophanemayaud/Dev/EPFL MA1/Machine Learning/cs-433-project-2-ml_fools/th_csv_labels/png_masks_emb/*.png')\n",
    "    !cp \"/Users/theophanemayaud/Dev/EPFL MA1/Machine Learning/cs-433-project-2-ml_fools/helpers.py\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIo9U3weoKDH",
    "outputId": "8788c4ce-e306-4f73-d0ec-419cfab39370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 357 images\n",
      "Found 357 labels\n"
     ]
    }
   ],
   "source": [
    "from helpers import png_to_mask, segment_dataset, compute_emb_surf_pred_error, confusion, out_predict\n",
    "\n",
    "imgs_names= sorted(imgs_names)\n",
    "#imgs = [cv.imread(name, cv.IMREAD_UNCHANGED) for name in imgs_names[1]]\n",
    "print(f\"Found {len(imgs_names)} images\")\n",
    "\n",
    "labels_names= sorted(labels_names)\n",
    "#labels = [png_to_mask(cv.imread(name, cv.IMREAD_UNCHANGED)) for name in labels_names]\n",
    "print(f\"Found {len(labels_names)} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters are here :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 197 test images\n",
      "Found 197 test labels\n"
     ]
    }
   ],
   "source": [
    "# Training configuration parameters\n",
    "learning_rate = 1e-3\n",
    "\n",
    "num_epochs=30\n",
    "number_of_images = 30\n",
    "train_img_pathnames = imgs_names[:number_of_images]\n",
    "train_label_pathnames = labels_names[:number_of_images]\n",
    "\n",
    "disp_every_x_epoch = 10\n",
    "disp_every_x_img = 10\n",
    "max_seg_to_disp = 5    # NB 1 (and not 0) means the first segment is displayed\n",
    "disp_every_x_seg = 3\n",
    "\n",
    "print_images_while_training = True\n",
    "\n",
    "# Testing predictions parameters\n",
    "# test_img_start = 300\n",
    "# test_img_stop = 330\n",
    "# test_img_pathnames = imgs_names[test_img_start:test_img_stop]\n",
    "# test_label_pathnames = labels_names[test_img_start:test_img_stop]\n",
    "test_imgs_names = glob.glob( '/home/jupyter/cs-433-project-2-ml_fools/th_analysedimages/Session2*.tif')\n",
    "test_labels_names = glob.glob('/home/jupyter/cs-433-project-2-ml_fools/th_csv_labels/png_masks_emb/Session2*.png')\n",
    "test_imgs_names = sorted(test_imgs_names)\n",
    "print(f\"Found {len(test_imgs_names)} test images\")\n",
    "test_labels_names = sorted(test_labels_names)\n",
    "print(f\"Found {len(test_labels_names)} test labels\")\n",
    "test_img_pathnames = test_imgs_names\n",
    "test_label_pathnames = test_labels_names\n",
    "\n",
    "\n",
    "print_each_test_stat = True\n",
    "print_each_test_img = False\n",
    "\n",
    "# Saving parameters when in train mode\n",
    "model_name = \"30epoch30firstImglr1e-3AdamCrossentr\"\n",
    "model_description = \" no specifics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK-k0p8PoKDH"
   },
   "source": [
    "UNet picture representation : \n",
    "\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRxFnJdmC707",
    "outputId": "290bd142-57fe-4df0-d60f-0e9cf8e6721b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device is: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the device to train on, if a GPU is available (ex when on Google Colab)\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Things will go much quicker if you enable a GPU, ex in Colab under 'Runtime / Change Runtime Type'\")\n",
    "else:\n",
    "    #del model # only needed when re-running multiple times\n",
    "    torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Selected device is: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fscs-r1JoKDH"
   },
   "outputs": [],
   "source": [
    "# UNet definitions\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # functions for going down the U\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.d_double_conv_1 = double_conv(1, 64)\n",
    "        self.d_double_conv_2 = double_conv(64, 128)\n",
    "        self.d_double_conv_3 = double_conv(128, 256)\n",
    "        self.d_double_conv_4 = double_conv(256, 512)\n",
    "        self.d_double_conv_5 = double_conv(512, 1024)\n",
    "        \n",
    "        # functions for going up the U\n",
    "        self.up_trans_4 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)        \n",
    "        self.u_double_conv_4 = double_conv(1024, 512)\n",
    "        self.up_trans_3 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
    "        self.u_double_conv_3 = double_conv(512, 256)\n",
    "        self.up_trans_2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
    "        self.u_double_conv_2 = double_conv(256, 128)\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
    "        self.u_double_conv_1 = double_conv(128, 64)\n",
    "        \n",
    "        self.out = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        '''makes the 388x388 prediction with the model, image must be 572x572pixels'''\n",
    "        \n",
    "        # Going down the U\n",
    "        d1 = self.d_double_conv_1(image) # first \"level\"\n",
    "        # print(x1.size())\n",
    "        x = self.max_pool_2x2(d1)\n",
    "        d2 = self.d_double_conv_2(x) # second\n",
    "        x = self.max_pool_2x2(d2)\n",
    "        d3 = self.d_double_conv_3(x) # third\n",
    "        x = self.max_pool_2x2(d3)\n",
    "        d4 = self.d_double_conv_4(x) # fourth\n",
    "        x = self.max_pool_2x2(d4)\n",
    "        x = self.d_double_conv_5(x) # last layer (fifth) : no max pool\n",
    "        # plt.imshow(x.detach().numpy()[0, 0, :, :])\n",
    "        \n",
    "        # Going up the U\n",
    "        x = self.up_trans_4(x)\n",
    "        d4 = crop_img(tensor=d4, target_tensor=x) #crop to copy\n",
    "        x = self.u_double_conv_4(torch.cat([d4, x], 1))\n",
    "        \n",
    "        x = self.up_trans_3(x)\n",
    "        d3 = crop_img(tensor=d3, target_tensor=x)\n",
    "        x = self.u_double_conv_3(torch.cat([d3, x], 1))\n",
    "        \n",
    "        x = self.up_trans_2(x)\n",
    "        d2 = crop_img(tensor=d2, target_tensor=x)\n",
    "        x = self.u_double_conv_2(torch.cat([d2, x], 1))\n",
    "        \n",
    "        x = self.up_trans_1(x)\n",
    "        d1 = crop_img(tensor=d1, target_tensor=x)\n",
    "        x = self.u_double_conv_1(torch.cat([d1, x], 1))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    \n",
    "# some functions so reduce redunduncy\n",
    "def double_conv(nb_in_channels, nb_out_channels): # Used for every descending step\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(nb_in_channels, nb_out_channels, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(nb_out_channels, nb_out_channels, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    return conv\n",
    "\n",
    "def crop_img(tensor, target_tensor): # Used for copy and crop between descending and ascending\n",
    "    target_size = target_tensor.size()[2] # NB they are square so .size[2]=.size[3]\n",
    "    tensor_size = tensor.size()[2]\n",
    "    delta = tensor_size - target_size #target is always smaller\n",
    "    pix_crop = delta // 2\n",
    "    return tensor[:, :, pix_crop:tensor_size-pix_crop, pix_crop:tensor_size-pix_crop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MPu2o3zAoKDI"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, img_pathnames, label_pathnames, criterion, optimizer, device, num_epochs=25, \n",
    "                disp_every_x_epoch = 1,\n",
    "                disp_every_x_img = 1,\n",
    "                max_seg_to_disp = 100, # NB 1 (and not 0) means the first segment is displayed\n",
    "                disp_every_x_seg = 1):\n",
    "    points=0 # incrementer for displaying progress\n",
    "    train_start_time = time.process_time() # For process monitoring\n",
    "\n",
    "    print(\"Starting the training on images !\")\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for image_i in range(len(img_pathnames)):\n",
    "            image = cv.imread(img_pathnames[image_i], cv.IMREAD_UNCHANGED)\n",
    "            label = png_to_mask(cv.imread(label_pathnames[image_i], cv.IMREAD_UNCHANGED))\n",
    "            \n",
    "            image_segments, label_segments = segment_dataset([image], [label])\n",
    "            \n",
    "            for segment_i in range(len(image_segments[:, 0, 0])):\n",
    "                since = time.process_time() # For process monitoring\n",
    "                img_seg = torch.tensor(image_segments[segment_i, :, :], requires_grad=True).view(1, 1, 572, 572).to(device).float()\n",
    "                label_seg = torch.tensor(label_segments[segment_i, :, :].astype(float)).view(1, 388, 388).to(device).long()\n",
    "                \n",
    "                prediction = model(img_seg)\n",
    "                loss = criterion(prediction, label_seg)\n",
    "            \n",
    "                # Compute the gradient\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update the parameters of the model with a gradient step\n",
    "                optimizer.step()\n",
    "                            \n",
    "                # Optionnaly display status during training\n",
    "                if ( epoch%disp_every_x_epoch==0 and image_i%disp_every_x_img==0 \n",
    "                        and segment_i%disp_every_x_seg==0 and segment_i<max_seg_to_disp ): \n",
    "                    if image_i==0 and segment_i==0:\n",
    "                        print(f\"\\nEpoch {1+epoch}/{num_epochs}\", end=\"\")\n",
    "                    if segment_i==0:\n",
    "                        print(f\"\\n|  Image {1+image_i}/{len(img_pathnames)} '{img_pathnames[image_i]}'\", end=\"\")\n",
    "                \n",
    "                    time_elapsed = time.process_time()-since\n",
    "\n",
    "                    # Make new prediction so compare before and after step\n",
    "                    new_prediction = model(img_seg)\n",
    "                    new_loss = criterion(new_prediction, label_seg)\n",
    "                    ori_lab_seg = label_seg.cpu().detach().numpy()[0,:,:].astype(int)\n",
    "                    pred_lab_seg = torch.argmax(prediction, dim=1).cpu().detach().numpy()[0, :, :]\n",
    "\n",
    "                    emb_surf_pred_error = compute_emb_surf_pred_error(ori_lab_seg, pred_lab_seg, print_values=False)\n",
    "                    print(f\"\\n|  |  Segment {1+segment_i}/{len(image_segments[:, 0, 0])} : loss={loss} \"+\\\n",
    "                          f\"duration={int(time_elapsed)//60}m {int(time_elapsed%60)}s. Loss reduced {loss-new_loss}. \"+\\\n",
    "                          f\"Emb surf pred err= {emb_surf_pred_error}%\", end=\"\")\n",
    "                        \n",
    "                    # # Also optionally print confusion values :\n",
    "                    # print(f\"\\n|  |  Confusion values are :{confusion(ori_lab_seg, pred_lab_seg, data_type='numpy')}\")\n",
    "                    \n",
    "                    # Also optionnaly print image segment, label and prediction\n",
    "                    if print_images_while_training == True:\n",
    "                        _ , axs = plt.subplots(ncols=3, figsize=(40, 40))\n",
    "                        axs[0].set_title(\"Segment of original image\")\n",
    "                        axs[0].imshow(img_seg.cpu().detach().numpy()[0,0,:,:])\n",
    "                        axs[1].set_title(\"Segment of original label\")\n",
    "                        axs[1].imshow(label_seg.cpu().detach().numpy()[0,:,:])\n",
    "                        axs[2].set_title(\"Predicted label segment\")\n",
    "                        axs[2].imshow(torch.argmax(prediction, dim=1).cpu().detach().numpy()[0,:,:])\n",
    "                        plt.show()\n",
    "                    \n",
    "                    points=0\n",
    "\n",
    "                if ( (epoch%disp_every_x_epoch==0 and image_i%disp_every_x_img==1 and segment_i==0) \n",
    "                        or (epoch%disp_every_x_epoch==1 and points==0) ):\n",
    "                    print(\"\\n   ...Next epochs & images \", end=\"\")\n",
    "                    points = 1\n",
    "                else:\n",
    "                    print(\".\", end=\"\")\n",
    "                    points = points+1 \n",
    "                    if  points>100:\n",
    "                        print(\"\\n      ...\", end=\"\")\n",
    "                        points = 1\n",
    "                        \n",
    "    time_elapsed = time.process_time()-train_start_time\n",
    "    print(f\"\\n\\n Finished training after {int(time_elapsed)//60}m {int(time_elapsed%60)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxvycGsWoKDJ",
    "outputId": "01ec10c1-12b5-4b63-9e77-dea48ed70fab"
   },
   "outputs": [],
   "source": [
    "# Actually training the model\n",
    "if train_or_load == \"train\" : \n",
    "    \n",
    "    model = UNet().to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_model(model, train_img_pathnames, train_label_pathnames, criterion, optimizer, device, num_epochs,\n",
    "                disp_every_x_epoch,\n",
    "                disp_every_x_img,\n",
    "                max_seg_to_disp,\n",
    "                disp_every_x_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nsFNNge0h3eZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters are :\n",
      "   name : 30epoch30firstImglr1e-3AdamCrossentr\n",
      "   description :  no specifics\n",
      "   Model details dump : env=gcp, train_or_load=train, learning rate=0.001, epochs=30, optimizer=Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      "), criterion=CrossEntropyLoss()\n",
      "Model successfully loaded !\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained model\n",
    "if train_or_load == \"load\":  \n",
    "    PATH = \"/content/drive/MyDrive/savedmodels/\" + model_name + \".pkl\"\n",
    "    if env == \"local\" or env == \"gcp\":\n",
    "        PATH = \"./savedmodels/\" + model_name + \".pkl\"\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        map_to_device = \"cuda:0\"\n",
    "    else:\n",
    "        map_to_device = \"cpu\"\n",
    "\n",
    "    model = UNet()\n",
    "\n",
    "    checkpoint = torch.load(PATH)\n",
    "\n",
    "    print(\"Model parameters are :\")\n",
    "    print(f\"   name : {checkpoint['name']}\")\n",
    "    print(f\"   description : {checkpoint['description']}\")\n",
    "    print(f\"   Model details dump : {checkpoint['model_details_dump']}\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # # If wanting to resume training :\n",
    "    # optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    model.to(device)\n",
    "    print(\"Model successfully loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "Sc0iyD5toKDJ",
    "outputId": "f656ba85-d1ab-47b6-ed43-0c2f139b3b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing predictions \n",
      "\n",
      "Emb surf pred err=-31.05333310837031%\n",
      "Confusion values are :(0.6826222416767516, 0.996571769755952, 0.0034282302440480665, 0.31737775832324844)\n",
      "Emb surf pred err=-6.713211600429646%\n",
      "Confusion values are :(0.7469803724207348, 0.9964321807380262, 0.0035678192619737997, 0.2530196275792652)\n",
      "Emb surf pred err=-20.744137116543108%\n",
      "Confusion values are :(0.7354249032162613, 0.9969789732577439, 0.0030210267422561264, 0.2645750967837387)\n",
      "Emb surf pred err=-6.706760048721072%\n",
      "Confusion values are :(0.732824427480916, 0.9962928109786643, 0.003707189021335753, 0.26717557251908397)\n",
      "Emb surf pred err=-1.9664108161397085%\n",
      "Confusion values are :(0.7864677750729442, 0.9954934332140101, 0.0045065667859899485, 0.2135322249270558)\n",
      "Emb surf pred err=18.41158232753848%\n",
      "Confusion values are :(0.7963403831247924, 0.9939966374655593, 0.006003362534440692, 0.20365961687520762)\n",
      "Emb surf pred err=-11.800669050640213%\n",
      "Confusion values are :(0.7359678085018572, 0.9985373128074643, 0.0014626871925357244, 0.2640321914981428)\n",
      "Emb surf pred err=-7.090828006540805%\n",
      "Confusion values are :(0.8026443642420877, 0.9981826313612712, 0.001817368638728852, 0.19735563575791226)\n",
      "Emb surf pred err=-49.62442218798151%\n",
      "Confusion values are :(0.5740490442170303, 0.9991823649799945, 0.0008176350200055476, 0.4259509557829697)\n",
      "Emb surf pred err=-15.7728151925444%\n",
      "Confusion values are :(0.7273693803159174, 0.9983943742306004, 0.001605625769399598, 0.27263061968408264)\n",
      "Emb surf pred err=0.948563794255177%\n",
      "Confusion values are :(0.7607882431529727, 0.997754654884485, 0.002245345115514955, 0.2392117568470274)\n",
      "Emb surf pred err=3.1693799792171804%\n",
      "Confusion values are :(0.8130700842858792, 0.9967988544514266, 0.0032011455485733897, 0.18692991571412076)\n",
      "Emb surf pred err=-9.196922926160028%\n",
      "Confusion values are :(0.720003589697568, 0.9984033433543695, 0.0015966566456304195, 0.279996410302432)\n",
      "Emb surf pred err=-16.13082845107248%\n",
      "Confusion values are :(0.7025162083845817, 0.9973750328250299, 0.0026249671749700907, 0.29748379161541827)\n",
      "Emb surf pred err=0.07966633862880172%\n",
      "Confusion values are :(0.749800834153428, 0.9976357678832575, 0.0023642321167425683, 0.250199165846572)\n",
      "Emb surf pred err=-43.826431483104116%\n",
      "Confusion values are :(0.03069954705586311, 0.9893754149897692, 0.010624585010230768, 0.9693004529441369)\n",
      "Emb surf pred err=-1.808490051545485%\n",
      "Confusion values are :(0.7625581033678354, 0.9944314815552956, 0.005568518444704432, 0.23744189663216464)\n",
      "Emb surf pred err=-10.294004166988193%\n",
      "Confusion values are :(0.6636115581053663, 0.99601668543281, 0.003983314567190054, 0.33638844189463374)\n",
      "Emb surf pred err=48.35759371377045%\n",
      "Confusion values are :(0.7967280690454721, 0.9965138045363694, 0.0034861954636306518, 0.2032719309545279)\n",
      "Emb surf pred err=-1.221575728789408%\n",
      "Confusion values are :(0.7289537356111152, 0.9941934287538541, 0.005806571246145885, 0.2710462643888848)\n",
      "Emb surf pred err=-11.243705413344523%\n",
      "Confusion values are :(0.7663326653306614, 0.9964693260141742, 0.0035306739858258257, 0.23366733466933867)\n",
      "Emb surf pred err=-11.918828666837914%\n",
      "Confusion values are :(0.6311682350240991, 0.9985019718140034, 0.0014980281859966475, 0.36883176497590087)\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "import pandas as pd\n",
    "def test_predictions(model, test_img_pathnames, test_label_pathnames):\n",
    "    emb_surf_pred_errors_list = []\n",
    "    emb_surf_TP_list = []\n",
    "    emb_surf_FN_list = []\n",
    "\n",
    "    print(\"Starting testing predictions \\n\")\n",
    "\n",
    "    for img_i in range(len(test_img_pathnames)):\n",
    "        test_img = cv.imread(test_img_pathnames[img_i], cv.IMREAD_UNCHANGED)\n",
    "        test_label = png_to_mask(cv.imread(test_label_pathnames[img_i], cv.IMREAD_UNCHANGED))\n",
    "\n",
    "        pred_label = out_predict(model, test_img, device)\n",
    "\n",
    "        emb_surf_pred_error = compute_emb_surf_pred_error(test_label, pred_label, print_values=False)\n",
    "        \n",
    "        TP, _, _, FN = confusion(pred_label, test_label, data_type='numpy')\n",
    "        emb_surf_TP_list.append(TP)\n",
    "        emb_surf_FN_list.append(FN)\n",
    "        \n",
    "        if print_each_test_stat == True:\n",
    "            print(f\"Emb surf pred err={emb_surf_pred_error}%\")\n",
    "            print(f\"Confusion values are :{confusion(pred_label, test_label, data_type='numpy')}\")\n",
    "        else:\n",
    "            print(\".\", end=\"\")\n",
    "        emb_surf_pred_errors_list.append(emb_surf_pred_error)\n",
    "\n",
    "        if img_i == len(test_img_pathnames)-1 or print_each_test_img == True:\n",
    "            print(\"\\nLast image looks like :\")\n",
    "            _ , axs = plt.subplots(ncols=3, figsize=(40, 40))\n",
    "\n",
    "            axs[0].set_title(\"Original image\")\n",
    "            axs[0].imshow(test_img)\n",
    "\n",
    "            axs[1].set_title(\"Original label\")\n",
    "            axs[1].imshow(test_label)\n",
    "\n",
    "            axs[2].set_title(\"Model predicted label\")\n",
    "            axs[2].imshow(pred_label)\n",
    "            plt.show()\n",
    "\n",
    "    # Draw histogram with boxplot of surface error values\n",
    "    sns.set(style=\"ticks\")\n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, \n",
    "                                        gridspec_kw={\"height_ratios\": (.15, .85)})\n",
    "    f.suptitle(f\"Surface error distribution for {len(test_img_pathnames)} test images\")\n",
    "    sns.boxplot(x=emb_surf_pred_errors_list, ax=ax_box, ci=None)\n",
    "    sns.histplot(x=emb_surf_pred_errors_list, ax=ax_hist, ci=None)\n",
    "    ax_box.set(yticks=[])\n",
    "    sns.despine(ax=ax_hist)\n",
    "    sns.despine(ax=ax_box, left=True)\n",
    "    plt.show()\n",
    "    plt.style.use('default')\n",
    "\n",
    "    print(f\"Finished testing with mean surface error={np.mean(emb_surf_pred_errors_list)}%\")\n",
    "    df = pd.DataFrame(data={\"Emb_surf_pred_err\":emb_surf_pred_errors_list, \"TP\": emb_surf_TP_list, \"TN\": emb_surf_FN_list})\n",
    "    df.to_csv(f\"./{model_name}.csv\", sep=',',index=False)\n",
    "    print(\"Saved CSV !\")\n",
    "    \n",
    "test_predictions(model, test_img_pathnames, test_label_pathnames)\n",
    "\n",
    "if train_or_load == \"train\":\n",
    "    print(\"\\n\\nCheck of performance on training set :\")\n",
    "    test_predictions(model, train_img_pathnames, train_label_pathnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "js_Ac9KAoKDK"
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "if train_or_load == \"train\":\n",
    "    print(f\"Remember to set a model name that makes it easy to identify\")\n",
    "    print(f\"Here are the notebook parameters : \\ncriterion={criterion} \\noptimizer={optimizer} \\nlr={learning_rate}, \\nepochs={num_epochs}\")\n",
    "    \n",
    "    PATH = \"/content/drive/MyDrive/savedmodels/\" + model_name + \".pkl\"\n",
    "    if env == \"local\" or env == \"gcp\":\n",
    "        PATH = \"./savedmodels/\" + model_name + \".pkl\"\n",
    "\n",
    "    print(\"\\nmodel_name = \" + model_name)\n",
    "    print(\"model_description = \" + model_description)\n",
    "    print(\"PATH = \" + PATH)\n",
    "    # Warning : only save once name and descriptions are set correctly\n",
    "    torch.save({\n",
    "            'name': model_name,\n",
    "            'description': model_description,\n",
    "            'model_details_dump': f\"env={env}, train_or_load={train_or_load}, learning rate={learning_rate}, epochs={num_epochs}, optimizer={optimizer}, criterion={criterion}\",         \n",
    "            'model_state_dict': model.state_dict(),\n",
    "            # 'optimizer_state_dict': optimizer.state_dict(), # Only needed when planning to resume training from loaded model\n",
    "            }, PATH)\n",
    "    print(\"Model saved !\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "th_u_net_ref_for_tests.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "common-cu110.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
