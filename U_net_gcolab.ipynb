{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the version of notebook which mount google drive on colab. So it is better to use the GPU of colab to train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mounting google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function can be found in helpers.py\n",
    "def png_to_mask(png):\n",
    "    '''convert nd array with 0 and 1 values to png image, transparent for 0, color of choice for 1\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    png : 4 or 3 (without alpha) dimensionnal nd array of 0 and color values (or transparency) of uint8 (max 255)\n",
    "    \n",
    "    output\n",
    "    ------\n",
    "    nd numpy array with 0 for background and 1 for color pixels\n",
    "    '''\n",
    "    # NB opencv works with RGBA, so 4 dimensions, with A being alpha, the transparency (0=transparent)\n",
    "    pngarray = np.array(png)\n",
    "#     print(pngarray[:,:,0])\n",
    "    mask = pngarray[:,:,0]\n",
    "    mask[pngarray[:,:,0]!=0]=1\n",
    "    mask[pngarray[:,:,1]!=0]=1\n",
    "    mask[pngarray[:,:,2]!=0]=1\n",
    "    if pngarray.shape[2]==4:\n",
    "        mask[pngarray[:,:,3]!=0]=1\n",
    "        \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using glob to read the images\n",
    "#On colab the names are non in alphabetic order so we sorted the names\n",
    "imgs_names = glob.glob('/content/drive/MyDrive/th_analysedimages/*.tif')\n",
    "imgs_names= sorted(imgs_names)\n",
    "#imgs = [cv.imread(name, cv.IMREAD_UNCHANGED) for name in imgs_names[1]]\n",
    "len(imgs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading labels names\n",
    "labels_names = glob.glob('/content/drive/MyDrive/labels/*.png')\n",
    "labels_names= sorted(labels_names)\n",
    "#labels = [png_to_mask(cv.imread(name, cv.IMREAD_UNCHANGED)) for name in labels_names]\n",
    "len(labels_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to cut a image of 'cut1' and 'cut2' pixels \n",
    "#It shouldn't be necessary for our training on segmentation\n",
    "def cut_y(img, cut1, cut2):\n",
    "    new_img=img\n",
    "    if cut1 != 0:\n",
    "        start=int(np.trunc(cut1/2)) + (cut1 % 2)\n",
    "        end= img.shape[0]-int(np.trunc(cut1/2))\n",
    "        new_img=new_img[start:end,:]\n",
    "    if cut2 != 0:\n",
    "        start=int(np.trunc(cut2/2)) + (cut2 % 2)\n",
    "        end= img.shape[1]-int(np.trunc(cut2/2))\n",
    "        new_img=new_img[:,start:end]\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for mirroring also this function is in helper.py\n",
    "def extend_mirror(img, out_size):\n",
    "    '''\n",
    "    A method to extend an image to certain resolution by mirrorring the edges\n",
    "    Input:\n",
    "    :img: image as numpy array\n",
    "    :out_size: a tuple of the desired output resolution\n",
    "    Output:\n",
    "    :out: the extended image\n",
    "    '''\n",
    "    # input error exceptions\n",
    "    if np.any(img.shape>tuple(out_size)):\n",
    "        raise Exception('Error: at least on of out_size axes is smaller than the image shape')\n",
    "    if np.any(3*img.shape>tuple(out_size)):\n",
    "        raise Exception('Error: at least on of out_size axes is at least 3 times larger than the image shape')\n",
    "    # output parameters\n",
    "    out = np.zeros(out_size)\n",
    "    v_edge_u = (out_size[0]-img.shape[0]) // 2\n",
    "    v_edge_d = -(out_size[0]-img.shape[0]-v_edge_u)\n",
    "    h_edge_l = (out_size[1]-img.shape[1]) // 2\n",
    "    h_edge_r = -(out_size[1]-img.shape[1]-h_edge_l)\n",
    "    # output centre\n",
    "    out[v_edge_u:v_edge_d,h_edge_l:h_edge_r] = img\n",
    "    # output sides\n",
    "    out[:v_edge_u,h_edge_l:h_edge_r] = np.flipud(img[:v_edge_u,:]) # top\n",
    "    out[v_edge_d:,h_edge_l:h_edge_r] = np.flipud(img[v_edge_d:,:]) # bottom\n",
    "    out[v_edge_u:v_edge_d,:h_edge_l] = np.fliplr(img[:,:h_edge_l]) # left\n",
    "    out[v_edge_u:v_edge_d,h_edge_r:] = np.fliplr(img[:,h_edge_r:]) # right\n",
    "    # output corners\n",
    "    out[:v_edge_u,:h_edge_l] = np.fliplr(out[:v_edge_u,h_edge_l:h_edge_l*2]) # top-left\n",
    "    out[:v_edge_u,h_edge_r:] = np.fliplr(out[:v_edge_u,2*h_edge_r:h_edge_r]) # top-right\n",
    "    out[v_edge_d:,:h_edge_l] = np.fliplr(out[v_edge_d:,h_edge_l:h_edge_l*2]) # bottom-left\n",
    "    out[v_edge_d:,h_edge_r:] = np.fliplr(out[v_edge_d:,2*h_edge_r:h_edge_r]) # bottom-right\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing U-net for our problem. Here all the 23 layers are used. Due to the fact that our problem consist in saying if a pixel is part or not of an air bubble the final output is given by tensor with two channels, each one corresponding to a matrix of dimensions equal to the original image. Using many convolutional layers the final output has a lower dimension than the input. In order to obtained the desired output the input image is expanded. The parts added are obtained mirroring parts of the real image as described in the paper. \n",
    "This tecnique is used in the training of NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definining the architecture of U-net in pytorch\n",
    "class U_net(nn.Module):\n",
    "    def __init__(self, n_channels=64):\n",
    "        'U-net from the paper \"Olaf Ronneberger, Philipp Fischer, and Thomas Brox\": https://arxiv.org/abs/1505.04597 '\n",
    "        super(U_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,n_channels,3)\n",
    "        self.conv2 = nn.Conv2d(n_channels,n_channels,3)\n",
    "        self.pool = nn.MaxPool2d(2,stride=2)\n",
    "        self.conv3 = nn.Conv2d(n_channels,2*n_channels,3)\n",
    "        self.conv4 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
    "        self.conv5 = nn.Conv2d(2*n_channels,4*n_channels,3)\n",
    "        self.conv6 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
    "        self.conv7 = nn.Conv2d(4*n_channels,8*n_channels,3)\n",
    "        self.conv8 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
    "        self.conv9 = nn.Conv2d(8*n_channels,16*n_channels,3)\n",
    "        self.conv10 = nn.Conv2d(16*n_channels,16*n_channels,3)\n",
    "        #self.upconv1= nn.ConvTranspose2d(16*n_channels,16*n_channels,2)\n",
    "        self.upconv1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv11 = nn.Conv2d(16*n_channels,8*n_channels,3)\n",
    "        self.conv12 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
    "        #self.upconv2= nn.ConvTranspose2d(8*n_channels,8*n_channels,2)\n",
    "        self.upconv2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv13 = nn.Conv2d(8*n_channels,4*n_channels,3)\n",
    "        self.conv14 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
    "        #self.upconv3= nn.ConvTranspose2d(4*n_channels,4*n_channels,2)\n",
    "        self.upconv3 = nn.Upsample(scale_factor=2)\n",
    "        self.conv15 = nn.Conv2d(4*n_channels,2*n_channels,3)\n",
    "        self.conv16 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
    "        #self.upconv4= nn.ConvTranspose2d(2*n_channels,2*n_channels,2)\n",
    "        self.upconv4 = nn.Upsample(scale_factor=2)\n",
    "        self.conv17 = nn.Conv2d(2*n_channels,n_channels,3)\n",
    "        self.conv18 = nn.Conv2d(n_channels,n_channels,3)\n",
    "        self.conv1_1= nn.Conv2d(n_channels,2,1)\n",
    "        self.activation1=nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.pool(self.activation1(self.conv2(self.activation1(self.conv1(x)))))\n",
    "        out = self.pool(self.activation1(self.conv4(self.activation1(self.conv3(out)))))\n",
    "        out = self.pool(self.activation1(self.conv6(self.activation1(self.conv5(out)))))\n",
    "        out = self.pool(self.activation1(self.conv8(self.activation1(self.conv7(out)))))\n",
    "        out = self.activation1(self.conv10(self.activation1(self.conv9(out))))\n",
    "        out = self.upconv1(out)\n",
    "        out = self.activation1(self.conv12(self.activation1(self.conv11(out))))\n",
    "        out = self.upconv2(out)\n",
    "        out = self.activation1(self.conv14(self.activation1(self.conv13(out))))\n",
    "        out = self.upconv3(out)\n",
    "        out = self.activation1(self.conv16(self.activation1(self.conv15(out))))\n",
    "        out = self.upconv4(out)\n",
    "        out = self.activation1(self.conv18(self.activation1(self.conv17(out))))\n",
    "        out = self.conv1_1(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "#The function select the 'correct' prediction (0 or 1) using argmax\n",
    "def accuracy(pred, test_labels):\n",
    "    '''\n",
    "    pred: torch.tensor (result of U-net) of size [num_batches=1, 2, dim_image1, dim_image2]\n",
    "    test_labels: torch.tensor (Real labels for the image) \n",
    "    '''\n",
    "    '''\n",
    "    Calculate the percentage of correct pixels labeled\n",
    "    '''\n",
    "    test_labels=test_labels.view(1,test_labels)\n",
    "    label_pred=torch.argmax(pred,dim=1)\n",
    "    acc= (torch.abs(label_pred-test_labels)).mean()\n",
    "    return 1-acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Trying the training for one images\n",
    "#This part is only to understand how GPU works on our U-net\n",
    "#The traing is done on a single image\n",
    "num_epochs=20\n",
    "learning_rate=0.001\n",
    "model=U_net()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate) #In the paper they use SGD\n",
    "criterion=torch.nn.CrossEntropyLoss() \n",
    "for p in model.parameters():\n",
    "    p.data = p.data.type(torch.cuda.DoubleTensor)\n",
    "print(\"Starting training\")\n",
    "# If a GPU is available (should be on Colab, we will use it)\n",
    "if not torch.cuda.is_available():\n",
    "  raise Exception(\"Things will go much quicker if you enable a GPU in Colab under 'Runtime / Change Runtime Type'\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Cycle for epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Train an epoch\n",
    "    model.train()\n",
    "    #Training using an image\n",
    "    im= cv.imread(imgs_names[18], cv.IMREAD_UNCHANGED)\n",
    "    \n",
    "    print(imgs_names[18])\n",
    "    y= png_to_mask(cv.imread(labels_names[18], cv.IMREAD_UNCHANGED))\n",
    "\n",
    "    print(im.shape, y.shape)\n",
    "    im=cut_y(im,33,99)\n",
    "    y=cut_y(y,157,223)\n",
    "    print(im.shape, y.shape)\n",
    "    out_size=[572,572]\n",
    "    #Mirroring like describe in the paper\n",
    "    ext_x = extend_mirror(im, out_size)\n",
    "    ext_x=torch.from_numpy(ext_x)\n",
    "    ext_x=ext_x.view(1,1,572,572)\n",
    "    ext_x=ext_x.to(device)\n",
    "    y=torch.from_numpy(y) \n",
    "    y = y.type(torch.LongTensor)\n",
    "    \n",
    "    print(y.size())\n",
    "    y=y.view(1,388,388)\n",
    "    y=y.to(device)\n",
    "\n",
    "    # Evaluate the network (forward pass)\n",
    "    prediction = model(ext_x)\n",
    "    loss = criterion(prediction,y)\n",
    "\n",
    "    # Compute the gradient\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters of the model with a gradient step\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
