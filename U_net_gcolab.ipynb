{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the version of notebook which mount google drive on colab. So it is better to use the GPU of colab to train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mounting google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_dataset(imgs, labels, in_size=572, out_size=388):\n",
    "    '''\n",
    "    A method to create a dataset ready to be used by a U-NET \n",
    "    Input:\n",
    "    :imgs: a list of images as uint16 numpy array\n",
    "    :labels: a list of masks of the embilized areas of the images as boolean numpy array\n",
    "    :in_size: axis size of U-NET inputs\n",
    "    :out_size: axis size of U-NET outputs\n",
    "    Output:\n",
    "    :X: a 3D numpy array of the inputs for the U-NET\n",
    "    :y: a 3D numpy array of the outputs for the U-NET\n",
    "    '''\n",
    "    X, y = [], [] # lists of input and output data respectively\n",
    "    ext = in_size - out_size # extand-mirror overall length\n",
    "    for i, img in enumerate(imgs): # run through all images\n",
    "        img_shp = np.array(img.shape) # store original image shape\n",
    "        img_aug = extend_mirror(img, img_shp+ext) # extand-mirror input image\n",
    "        segs = np.ceil(img_shp / out_size) # number of segments in each axis\n",
    "        vg,hg = np.meshgrid(np.arange(segs[0]),np.arange(segs[1])) # create a grid of each axis\n",
    "        grid = np.array([vg.ravel(),hg.ravel()]).T.astype(np.uint8) # create an array of segments coordinates\n",
    "        for vh in grid: # run for each segment coordinate\n",
    "            X_start = np.rint(vh*img_shp/segs + (ext/2 - in_size/2)*((0<vh)+(segs<=vh))).astype(np.uint16) # start pixel of input\n",
    "            y_start = np.rint(vh*img_shp/segs - (out_size/2)*((0<vh)+(segs<=vh))).astype(np.uint16) # start pixel of output\n",
    "            Xi = img_aug[X_start[0]:X_start[0]+in_size, X_start[1]:X_start[1]+in_size] # extract input segment\n",
    "            yi = labels[i][y_start[0]:y_start[0]+out_size, y_start[1]:y_start[1]+out_size] # extract output segment\n",
    "            X.append(Xi) # add to inputs list\n",
    "            y.append(yi) # add to outputs list\n",
    "    return np.array(X), np.array(y) # convert to np.array and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function can be found in helpers.py\n",
    "def png_to_mask(png):\n",
    "    '''convert nd array with 0 and 1 values to png image, transparent for 0, color of choice for 1\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    png : 4 or 3 (without alpha) dimensionnal nd array of 0 and color values (or transparency) of uint8 (max 255)\n",
    "    \n",
    "    output\n",
    "    ------\n",
    "    nd numpy array with 0 for background and 1 for color pixels\n",
    "    '''\n",
    "    # NB opencv works with RGBA, so 4 dimensions, with A being alpha, the transparency (0=transparent)\n",
    "    pngarray = np.array(png)\n",
    "#     print(pngarray[:,:,0])\n",
    "    mask = pngarray[:,:,0]\n",
    "    mask[pngarray[:,:,0]!=0]=1\n",
    "    mask[pngarray[:,:,1]!=0]=1\n",
    "    mask[pngarray[:,:,2]!=0]=1\n",
    "    if pngarray.shape[2]==4:\n",
    "        mask[pngarray[:,:,3]!=0]=1\n",
    "        \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using glob to read the images\n",
    "#On colab the names are non in alphabetic order so we sorted the names\n",
    "imgs_names = glob.glob('/content/drive/MyDrive/th_analysedimages/*.tif')\n",
    "imgs_names= sorted(imgs_names)\n",
    "#imgs = [cv.imread(name, cv.IMREAD_UNCHANGED) for name in imgs_names[1]]\n",
    "len(imgs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading labels names\n",
    "labels_names = glob.glob('/content/drive/MyDrive/labels/*.png')\n",
    "labels_names= sorted(labels_names)\n",
    "#labels = [png_to_mask(cv.imread(name, cv.IMREAD_UNCHANGED)) for name in labels_names]\n",
    "len(labels_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to cut a image of 'cut1' and 'cut2' pixels \n",
    "#It shouldn't be necessary for our training on segmentation\n",
    "def cut_y(img, cut1, cut2):\n",
    "    new_img=img\n",
    "    if cut1 != 0:\n",
    "        start=int(np.trunc(cut1/2)) + (cut1 % 2)\n",
    "        end= img.shape[0]-int(np.trunc(cut1/2))\n",
    "        new_img=new_img[start:end,:]\n",
    "    if cut2 != 0:\n",
    "        start=int(np.trunc(cut2/2)) + (cut2 % 2)\n",
    "        end= img.shape[1]-int(np.trunc(cut2/2))\n",
    "        new_img=new_img[:,start:end]\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for mirroring also this function is in helper.py\n",
    "def extend_mirror(img, out_size):\n",
    "    '''\n",
    "    A method to extend an image to certain resolution by mirrorring the edges\n",
    "    Input:\n",
    "    :img: image as numpy array\n",
    "    :out_size: a tuple of the desired output resolution\n",
    "    Output:\n",
    "    :out: the extended image\n",
    "    '''\n",
    "    # input error exceptions\n",
    "    if np.any(img.shape>tuple(out_size)):\n",
    "        raise Exception('Error: at least on of out_size axes is smaller than the image shape')\n",
    "    if np.any(3*img.shape>tuple(out_size)):\n",
    "        raise Exception('Error: at least on of out_size axes is at least 3 times larger than the image shape')\n",
    "    # output parameters\n",
    "    out = np.zeros(out_size)\n",
    "    v_edge_u = (out_size[0]-img.shape[0]) // 2\n",
    "    v_edge_d = -(out_size[0]-img.shape[0]-v_edge_u)\n",
    "    h_edge_l = (out_size[1]-img.shape[1]) // 2\n",
    "    h_edge_r = -(out_size[1]-img.shape[1]-h_edge_l)\n",
    "    # output centre\n",
    "    out[v_edge_u:v_edge_d,h_edge_l:h_edge_r] = img\n",
    "    # output sides\n",
    "    out[:v_edge_u,h_edge_l:h_edge_r] = np.flipud(img[:v_edge_u,:]) # top\n",
    "    out[v_edge_d:,h_edge_l:h_edge_r] = np.flipud(img[v_edge_d:,:]) # bottom\n",
    "    out[v_edge_u:v_edge_d,:h_edge_l] = np.fliplr(img[:,:h_edge_l]) # left\n",
    "    out[v_edge_u:v_edge_d,h_edge_r:] = np.fliplr(img[:,h_edge_r:]) # right\n",
    "    # output corners\n",
    "    out[:v_edge_u,:h_edge_l] = np.fliplr(out[:v_edge_u,h_edge_l:h_edge_l*2]) # top-left\n",
    "    out[:v_edge_u,h_edge_r:] = np.fliplr(out[:v_edge_u,2*h_edge_r:h_edge_r]) # top-right\n",
    "    out[v_edge_d:,:h_edge_l] = np.fliplr(out[v_edge_d:,h_edge_l:h_edge_l*2]) # bottom-left\n",
    "    out[v_edge_d:,h_edge_r:] = np.fliplr(out[v_edge_d:,2*h_edge_r:h_edge_r]) # bottom-right\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing U-net for our problem. Here all the 23 layers are used. Due to the fact that our problem consist in saying if a pixel is part or not of an air bubble the final output is given by tensor with two channels, each one corresponding to a matrix of dimensions equal to the original image. Using many convolutional layers the final output has a lower dimension than the input. In order to obtained the desired output the input image is expanded. The parts added are obtained mirroring parts of the real image as described in the paper. \n",
    "This tecnique is used in the training of NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definining the architecture of U-net in pytorch\n",
    "class U_net(nn.Module):\n",
    "    def __init__(self, n_channels=64):\n",
    "        'U-net from the paper \"Olaf Ronneberger, Philipp Fischer, and Thomas Brox\": https://arxiv.org/abs/1505.04597 '\n",
    "        super(U_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,n_channels,3)\n",
    "        self.conv2 = nn.Conv2d(n_channels,n_channels,3)\n",
    "        self.pool = nn.MaxPool2d(2,stride=2)\n",
    "        self.conv3 = nn.Conv2d(n_channels,2*n_channels,3)\n",
    "        self.conv4 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
    "        self.conv5 = nn.Conv2d(2*n_channels,4*n_channels,3)\n",
    "        self.conv6 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
    "        self.conv7 = nn.Conv2d(4*n_channels,8*n_channels,3)\n",
    "        self.conv8 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
    "        self.conv9 = nn.Conv2d(8*n_channels,16*n_channels,3)\n",
    "        self.conv10 = nn.Conv2d(16*n_channels,16*n_channels,3)\n",
    "        #self.upconv1= nn.ConvTranspose2d(16*n_channels,16*n_channels,2)\n",
    "        self.upconv1 = nn.Upsample(scale_factor=2)\n",
    "        self.conv11 = nn.Conv2d(16*n_channels,8*n_channels,3)\n",
    "        self.conv12 = nn.Conv2d(8*n_channels,8*n_channels,3)\n",
    "        #self.upconv2= nn.ConvTranspose2d(8*n_channels,8*n_channels,2)\n",
    "        self.upconv2 = nn.Upsample(scale_factor=2)\n",
    "        self.conv13 = nn.Conv2d(8*n_channels,4*n_channels,3)\n",
    "        self.conv14 = nn.Conv2d(4*n_channels,4*n_channels,3)\n",
    "        #self.upconv3= nn.ConvTranspose2d(4*n_channels,4*n_channels,2)\n",
    "        self.upconv3 = nn.Upsample(scale_factor=2)\n",
    "        self.conv15 = nn.Conv2d(4*n_channels,2*n_channels,3)\n",
    "        self.conv16 = nn.Conv2d(2*n_channels,2*n_channels,3)\n",
    "        #self.upconv4= nn.ConvTranspose2d(2*n_channels,2*n_channels,2)\n",
    "        self.upconv4 = nn.Upsample(scale_factor=2)\n",
    "        self.conv17 = nn.Conv2d(2*n_channels,n_channels,3)\n",
    "        self.conv18 = nn.Conv2d(n_channels,n_channels,3)\n",
    "        self.conv1_1= nn.Conv2d(n_channels,2,1)\n",
    "        self.activation1=nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.pool(self.activation1(self.conv2(self.activation1(self.conv1(x)))))\n",
    "        out = self.pool(self.activation1(self.conv4(self.activation1(self.conv3(out)))))\n",
    "        out = self.pool(self.activation1(self.conv6(self.activation1(self.conv5(out)))))\n",
    "        out = self.pool(self.activation1(self.conv8(self.activation1(self.conv7(out)))))\n",
    "        out = self.activation1(self.conv10(self.activation1(self.conv9(out))))\n",
    "        out = self.upconv1(out)\n",
    "        out = self.activation1(self.conv12(self.activation1(self.conv11(out))))\n",
    "        out = self.upconv2(out)\n",
    "        out = self.activation1(self.conv14(self.activation1(self.conv13(out))))\n",
    "        out = self.upconv3(out)\n",
    "        out = self.activation1(self.conv16(self.activation1(self.conv15(out))))\n",
    "        out = self.upconv4(out)\n",
    "        out = self.activation1(self.conv18(self.activation1(self.conv17(out))))\n",
    "        out = self.conv1_1(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialiazing weights\n",
    "torch.nn.init.normal_(U_net.conv1, mean=0.0, std=np.sqrt(2/576))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "#The function select the 'correct' prediction (0 or 1) using argmax\n",
    "def accuracy(pred, test_labels):\n",
    "    '''\n",
    "    pred: torch.tensor (result of U-net) of size [num_batches=1, 2, dim_image1, dim_image2]\n",
    "    test_labels: torch.tensor (Real labels for the image) \n",
    "    '''\n",
    "    '''\n",
    "    Calculate the percentage of correct pixels labeled\n",
    "    '''\n",
    "    test_labels=test_labels.view(1,test_labels)\n",
    "    label_pred=torch.argmax(pred,dim=1)\n",
    "    acc= (torch.abs(label_pred-test_labels)).mean()\n",
    "    return 1-acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Trying the training for one images\n",
    "#This part is only to understand how GPU works on our U-net\n",
    "#The traing is done on a single image\n",
    "num_epochs=20\n",
    "learning_rate=0.001\n",
    "model=U_net()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate) #In the paper they use SGD\n",
    "criterion=torch.nn.CrossEntropyLoss() \n",
    "for p in model.parameters():\n",
    "    p.data = p.data.type(torch.cuda.DoubleTensor)\n",
    "print(\"Starting training\")\n",
    "# If a GPU is available (should be on Colab, we will use it)\n",
    "if not torch.cuda.is_available():\n",
    "  raise Exception(\"Things will go much quicker if you enable a GPU in Colab under 'Runtime / Change Runtime Type'\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Cycle for epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Train an epoch\n",
    "    model.train()\n",
    "    #Training using an image\n",
    "    im= cv.imread(imgs_names[18], cv.IMREAD_UNCHANGED)\n",
    "    \n",
    "    print(imgs_names[18])\n",
    "    y= png_to_mask(cv.imread(labels_names[18], cv.IMREAD_UNCHANGED))\n",
    "\n",
    "    print(im.shape, y.shape)\n",
    "    im=cut_y(im,33,99)\n",
    "    y=cut_y(y,157,223)\n",
    "    print(im.shape, y.shape)\n",
    "    out_size=[572,572]\n",
    "    #Mirroring like describe in the paper\n",
    "    ext_x = extend_mirror(im, out_size)\n",
    "    ext_x=torch.from_numpy(ext_x)\n",
    "    ext_x=ext_x.view(1,1,572,572)\n",
    "    ext_x=ext_x.to(device)\n",
    "    y=torch.from_numpy(y) \n",
    "    y = y.type(torch.LongTensor)\n",
    "    \n",
    "    print(y.size())\n",
    "    y=y.view(1,388,388)\n",
    "    y=y.to(device)\n",
    "\n",
    "    # Evaluate the network (forward pass)\n",
    "    prediction = model(ext_x)\n",
    "    loss = criterion(prediction,y)\n",
    "\n",
    "    # Compute the gradient\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters of the model with a gradient step\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training (adapting function from lab10)\n",
    "def train(model, criterion, imgs_names, labels_names, optimizer, num_epochs):\n",
    "  \"\"\"\n",
    "  @param model: torch.nn.Module\n",
    "  @param criterion: torch.nn.modules.loss._Loss\n",
    "  @param image_input: numpy.ndarray\n",
    "  @param labeled_images: numpy.ndarray\n",
    "  @param optimizer: torch.optim.Optimizer\n",
    "  @param num_epochs: int\n",
    "  \"\"\"\n",
    "  # If a GPU is available (should be on Colab, we will use it)\n",
    "  if not torch.cuda.is_available():\n",
    "    raise Exception(\"Things will go much quicker if you enable a GPU in Colab under 'Runtime / Change Runtime Type'\")\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  print(\"Starting training\")\n",
    "  #Cycle for epochs\n",
    "  for epoch in range(num_epochs):\n",
    "    # Train an epoch\n",
    "    model.train()\n",
    "    print('epoch: {epoch}',epoch)\n",
    "    #for i in range(len(imgs_names)):\n",
    "    for i in range(30):\n",
    "      im= cv.imread(imgs_names[i], cv.IMREAD_UNCHANGED)\n",
    "      label= png_to_mask(cv.imread(labels_names[i], cv.IMREAD_UNCHANGED))\n",
    "      \n",
    "      X, y = segment_dataset([im], [label], in_size=572, out_size=388)\n",
    "      for j in range(len(X[:,0,0])):\n",
    "        X_input=torch.from_numpy(X[j,:,:])\n",
    "        X_input=X_input.view(1,1,572,572)\n",
    "\n",
    "        #X_input on GPU\n",
    "        X_input=X_input.to(device)\n",
    "\n",
    "        y_input=torch.from_numpy(y[j,:,:])\n",
    "        y_input=y_input.view(1,388,388)\n",
    "        y_input = y_input.type(torch.LongTensor)\n",
    "\n",
    "        #y_input on GPU\n",
    "        y_input=y_input.to(device)\n",
    "\n",
    "        # Evaluate the network (forward pass)\n",
    "        prediction = model(X_input)\n",
    "        loss = criterion(prediction,y_input)\n",
    "        \n",
    "        # Compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters of the model with a gradient step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "learning_rate=0.001\n",
    "model=U_net()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate) #In the paper they use SGD\n",
    "criterion=torch.nn.CrossEntropyLoss() \n",
    "for p in model.parameters():\n",
    "    p.data = p.data.type(torch.cuda.DoubleTensor)\n",
    "train(model, criterion, imgs_names, labels_names, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(x,y):\n",
    "  x_s, y_s = segment_dataset([x], [y], in_size=572, out_size=388)\n",
    "  for i in range(len(x_s[:,0,0])):\n",
    "    x_s_i = x_s[i,:,:]\n",
    "    x_s_i=torch.from_numpy(x_s_i)\n",
    "    x_s_i=x_s_i.view(1,1,572,572)\n",
    "    #print(x_s_i)\n",
    "    x_s_i = x_s_i.type(torch.DoubleTensor)\n",
    "    x_s_i = torch.tensor(x_s_i, dtype=torch.float32)\n",
    "    model=U_net()\n",
    "    predic = model(x_s_i)\n",
    "    print(predic)\n",
    "    label_pred=torch.argmin(predic,dim=1)\n",
    "    label_pred=label_pred.numpy()[::-1,:,:]\n",
    "    plt.figure()\n",
    "    plt.imshow(label_pred[0])\n",
    "    y_s_i = y_s[i,:,:]\n",
    "    #plt.imshow(y_s_i)\n",
    "    test_label=torch.from_numpy(y_s_i)\n",
    "    test_label=test_label.view(1,388,388)\n",
    "    #print(accuracy(predic, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im= cv.imread(imgs_names[99], cv.IMREAD_UNCHANGED)\n",
    "label= png_to_mask(cv.imread(labels_names[99], cv.IMREAD_UNCHANGED))\n",
    "#model.eval()\n",
    "#model=model.float()\n",
    "testing(im,label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
